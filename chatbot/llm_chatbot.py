import os
from openai import OpenAI
from dotenv import load_dotenv
import requests
import pandas as pd
import json
import time


load_dotenv('../.env')


client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

class LLMProjectChatbot:
    def __init__(self, api_url="http://localhost:8000"):
        self.api_url = api_url
        self.current_user = None
        self.project_data = {}
        self.questions_asked = 0  # Contador para evitar cumprimentos repetitivos
        
    def get_llm_response(self, user_message, context=""):
        """Usar GPT-4o-mini para respostas inteligentes"""
        system_prompt = f"""
        Voc√™ √© um assistente especialista em gest√£o de projetos chamado ProjectAI.
        Seu objetivo √© ajudar gestores a avaliar o sucesso de projetos usando dados e IA.
        
        Contexto: {context}
        
        Seja conversacional, profissional e fa√ßa perguntas de forma natural.
        Quando coletar dados, explique brevemente por que cada informa√ß√£o √© importante.
        Use emojis para tornar a conversa mais amig√°vel.
        
        IMPORTANTE: N√£o repita cumprimentos como "Ol√°" se j√° foi estabelecido o contexto da conversa.
        """
        
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                max_tokens=200,
                temperature=0.7
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"ü§ñ Ops, estou com dificuldades t√©cnicas, mas vamos continuar!"
    
    def intelligent_welcome(self):
        """Boas-vindas inteligentes com LLM"""
        print("ü§ñ " + "="*60)
        print("üéØ CHATBOT INTELIGENTE DE AN√ÅLISE DE PROJETOS")
        print("="*60)
        
        welcome_msg = """
        Seja bem-vindo! Sou o ProjectAI, seu assistente inteligente.
        
        Vou ajudar voc√™ a prever o sucesso do seu pr√≥ximo projeto usando:
        ‚Ä¢ Machine Learning treinado com 1000 projetos
        ‚Ä¢ An√°lise de IA contextual
        ‚Ä¢ Recomenda√ß√µes personalizadas
        
        Primeiro, preciso te conhecer melhor! üòä
        """
        
        response = self.get_llm_response(welcome_msg)
        print(f"\nü§ñ: {response}")
    
    def authenticate_user(self):
        """Autentica√ß√£o de usu√°rio"""
        try:
            users_df = pd.read_csv('../ml_model/data/users_data.csv')
            
            print("\nüë• Usu√°rios dispon√≠veis:")
            for _, user in users_df.iterrows():
                print(f"  {user['usuario_id']}. {user['nome']} ({user['cargo']})")
            
            while True:
                try:
                    user_id = int(input("\nüîë Digite seu ID de usu√°rio: "))
                    user = users_df[users_df['usuario_id'] == user_id]
                    
                    if not user.empty:
                        self.current_user = user.iloc[0].to_dict()
                        
                        # Cumprimento personalizado
                        greeting_context = f"Usu√°rio: {self.current_user['nome']}, {self.current_user['cargo']}, {self.current_user['experiencia_anos']} anos de experi√™ncia"
                        greeting_msg = f"O usu√°rio {self.current_user['nome']} acabou de fazer login. Cumprimente-o e comente sobre sua experi√™ncia."
                        
                        greeting = self.get_llm_response(greeting_msg, greeting_context)
                        print(f"\nü§ñ: {greeting}")
                        return True
                    else:
                        print("‚ùå Usu√°rio n√£o encontrado. Tente novamente.")
                        
                except ValueError:
                    print("‚ùå Digite um n√∫mero v√°lido.")
        except Exception as e:
            print(f"‚ùå Erro ao carregar usu√°rios: {e}")
            return False
    
    def get_question_for_field(self, field, config):
        """Gerar pergunta espec√≠fica para cada campo sem cumprimentos repetitivos"""
        
        # Perguntas pr√©-definidas para evitar repeti√ß√£o de cumprimentos
        predefined_questions = {
            'duracao_meses': "‚è±Ô∏è Quantos meses o projeto vai durar? Esta informa√ß√£o √© crucial para planejar marcos e avaliar o cronograma.",
            'orcamento': "üí∞ Qual o or√ßamento total do projeto em R$? O or√ßamento nos ajuda a avaliar a viabilidade e gerenciar recursos.",
            'tamanho_equipe': "üë• Quantas pessoas v√£o trabalhar no projeto? O tamanho da equipe impacta diretamente na capacidade de entrega.",
            'experiencia_gerente': "üéì Quantos anos de experi√™ncia tem o gerente? A experi√™ncia √© fundamental para a lideran√ßa e tomada de decis√µes.",
            'recursos_disponiveis': "üõ†Ô∏è Como voc√™ avalia os recursos dispon√≠veis? Isso inclui pessoal, tecnologia e ferramentas necess√°rias.",
            'complexidade': "üéØ Qual a complexidade t√©cnica do projeto? Complexidade maior pode exigir mais tempo e especializa√ß√£o.",
            'tipo_projeto': "üèóÔ∏è Qual o tipo de projeto? Diferentes tipos t√™m caracter√≠sticas e desafios espec√≠ficos."
        }
        
        # Usar pergunta pr√©-definida se dispon√≠vel
        if field in predefined_questions:
            return predefined_questions[field]
        
        # Fallback para gera√ß√£o din√¢mica (sem cumprimentos se j√° passaram da primeira pergunta)
        if self.questions_asked > 0:
            question_prompt = f"Fa√ßa uma pergunta direta sobre: {config['question']}. Explique brevemente por que √© importante. N√ÉO use cumprimentos como 'Ol√°' ou 'Oi'."
        else:
            question_prompt = f"Fa√ßa uma pergunta natural sobre: {config['question']}. Explique brevemente por que essa informa√ß√£o √© importante para an√°lise."
        
        context = f"Coletando {field} para an√°lise de projeto. Usu√°rio: {self.current_user['nome']}. Pergunta n√∫mero: {self.questions_asked + 1}"
        
        return self.get_llm_response(question_prompt, context)
    
    def collect_project_data(self):
        """Coleta dados do projeto com conversa√ß√£o natural"""
        print("\n" + "="*50)
        print("üìã COLETA DE DADOS DO PROJETO")
        print("="*50)
        
        # Mapeamento de campos
        field_map = {
            'duracao_meses': {'question': 'Quantos meses o projeto vai durar?', 'type': 'int'},
            'orcamento': {'question': 'Qual o or√ßamento total do projeto em R$?', 'type': 'float'},
            'tamanho_equipe': {'question': 'Quantas pessoas v√£o trabalhar no projeto?', 'type': 'int'},
            'experiencia_gerente': {'question': 'Quantos anos de experi√™ncia tem o gerente?', 'type': 'int'},
            'recursos_disponiveis': {'question': 'Recursos dispon√≠veis?', 'type': 'choice', 'options': ['Baixo', 'M√©dio', 'Alto']},
            'complexidade': {'question': 'Complexidade t√©cnica?', 'type': 'choice', 'options': ['Baixa', 'M√©dia', 'Alta']},
            'tipo_projeto': {'question': 'Tipo do projeto?', 'type': 'choice', 'options': ['TI', 'Constru√ß√£o', 'Marketing', 'P&D']}
        }
        
        self.questions_asked = 0
        
        for field, config in field_map.items():
            # Gerar pergunta inteligente
            question = self.get_question_for_field(field, config)
            print(f"\nü§ñ: {question}")
            
            self.questions_asked += 1
            
            # Coletar resposta
            while True:
                try:
                    if config['type'] == 'choice':
                        print(f"   Op√ß√µes: {', '.join(config['options'])}")
                    
                    user_input = input("üë§: ").strip()
                    
                    # Processar resposta
                    if config['type'] == 'int':
                        value = int(user_input)
                        self.project_data[field] = value
                        print(f"‚úÖ Registrado: {value}")
                        break
                    elif config['type'] == 'float':
                        # Limpar formata√ß√£o brasileira
                        clean_input = user_input.replace('R$', '').replace('.', '').replace(',', '.')
                        value = float(clean_input)
                        self.project_data[field] = value
                        print(f"‚úÖ Registrado: R$ {value:,.2f}")
                        break
                    elif config['type'] == 'choice':
                        # Compara√ß√£o case-insensitive
                        user_choice = user_input.strip().lower()
                        options_lower = [opt.lower() for opt in config['options']]
                        
                        if user_choice in options_lower:
                            # Encontrar a op√ß√£o original
                            original_option = config['options'][options_lower.index(user_choice)]
                            self.project_data[field] = original_option
                            print(f"‚úÖ Selecionado: {original_option}")
                            break
                        else:
                            print(f"‚ùå Escolha uma das op√ß√µes: {', '.join(config['options'])}")
                            print(f"üí° Dica: Digite exatamente como mostrado ou use min√∫sculas")
                    
                except ValueError:
                    print("‚ùå Formato inv√°lido. Tente novamente.")
        
        return True
    
    def display_project_summary(self):
        """Mostrar resumo do projeto"""
        print("\n" + "="*50)
        print("üìã RESUMO DO PROJETO")
        print("="*50)
        
        summary_context = f"Dados coletados: {self.project_data}. Usu√°rio: {self.current_user['nome']}"
        summary_prompt = "Fa√ßa um resumo amig√°vel e conciso dos dados do projeto que coletamos. Seja positivo e mencione se algo chama aten√ß√£o. N√ÉO use cumprimentos repetitivos."
        
        summary = self.get_llm_response(summary_prompt, summary_context)
        print(f"\nü§ñ: {summary}")
        
        # Dados t√©cnicos
        print(f"\nüìä DADOS T√âCNICOS:")
        print(f"   ‚è±Ô∏è  Dura√ß√£o: {self.project_data['duracao_meses']} meses")
        print(f"   üí∞ Or√ßamento: R$ {self.project_data['orcamento']:,.2f}")
        print(f"   üë• Equipe: {self.project_data['tamanho_equipe']} pessoas")
        print(f"   üîß Recursos: {self.project_data['recursos_disponiveis']}")
        print(f"   üéØ Complexidade: {self.project_data['complexidade']}")
        print(f"   üéì Exp. Gerente: {self.project_data['experiencia_gerente']} anos")
        print(f"   üèóÔ∏è  Tipo: {self.project_data['tipo_projeto']}")
    
    def get_ai_analysis(self):
        """Obter an√°lise completa ML + LLM"""
        print("\nüîÆ ANALISANDO COM IA...")
        print("-" * 30)
        
        # Anima√ß√£o de loading
        for i in range(3):
            print("ü§ñ Processando", "." * (i+1), end="\r")
            time.sleep(0.8)
        print("ü§ñ An√°lise conclu√≠da! ‚úÖ    ")
        
        try:
            # Chamar endpoint h√≠brido
            response = requests.post(
                f"{self.api_url}/analyze-with-llm",
                json=self.project_data,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()
            else:
                print(f"‚ùå Erro na API: {response.status_code}")
                return None
                
        except requests.exceptions.ConnectionError:
            print("‚ùå Erro: API n√£o est√° rodando!")
            print("üí° Execute: cd api && python app.py")
            return None
        except Exception as e:
            print(f"‚ùå Erro: {e}")
            return None
    
    def display_results(self, analysis):
        """Exibir resultados da an√°lise"""
        print("\n" + "="*60)
        print("üìä RESULTADO DA AN√ÅLISE INTELIGENTE")
        print("="*60)
        
        ml_pred = analysis['ml_prediction']
        prob = ml_pred['probabilidade_sucesso']
        
        # Status visual
        status = "SUCESSO ‚úÖ" if ml_pred['sucesso_previsto'] else "ATEN√á√ÉO ‚ö†Ô∏è"
        emoji = "üéâ" if ml_pred['sucesso_previsto'] else "‚ö†Ô∏è"
        
        print(f"\n{emoji} STATUS: {status}")
        print(f"üìà Probabilidade de Sucesso: {prob:.1%}")
        print(f"üéØ Confian√ßa: {ml_pred['confianca']}")
        
        # Barra visual
        bar_length = 20
        filled = int(prob * bar_length)
        bar = "‚ñà" * filled + "‚ñë" * (bar_length - filled)
        print(f"üìä [{bar}] {prob:.1%}")
        
        # An√°lise LLM
        print(f"\nü§ñ AN√ÅLISE ESPECIALISTA:")
        print("=" * 40)
        print(analysis['llm_analysis'])
        
        # Recomenda√ß√µes ML
        if ml_pred['recomendacoes']:
            print(f"\nüí° RECOMENDA√á√ïES R√ÅPIDAS:")
            for i, rec in enumerate(ml_pred['recomendacoes'], 1):
                print(f"{i}. {rec}")
        
        print(f"\n‚ú® {analysis['combined_insights']}")
    
    def run(self):
        """Executar chatbot inteligente"""
        try:
            # Verificar API
            try:
                response = requests.get(f"{self.api_url}/health", timeout=5)
                if response.status_code != 200:
                    print("‚ùå API n√£o est√° funcionando!")
                    return
            except:
                print("‚ùå API n√£o est√° rodando!")
                print("üí° Execute: cd api && python app.py")
                return
            
            # Fluxo principal
            self.intelligent_welcome()
            
            if not self.authenticate_user():
                return
            
            while True:
                print(f"\n{'='*60}")
                print("üöÄ NOVA AN√ÅLISE DE PROJETO")
                print("="*60)
                
                # Resetar contador de perguntas para novo projeto
                self.questions_asked = 0
                
                if not self.collect_project_data():
                    break
                
                self.display_project_summary()
                
                confirm = input(f"\n‚ùì Analisar este projeto com IA? (s/n): ").lower().strip()
                
                if confirm == 's':
                    analysis = self.get_ai_analysis()
                    if analysis:
                        self.display_results(analysis)
                    else:
                        print("‚ùå N√£o foi poss√≠vel fazer a an√°lise.")
                
                continue_analysis = input(f"\n‚ùì Analisar outro projeto? (s/n): ").lower().strip()
                if continue_analysis != 's':
                    break
                
                self.project_data = {}
            
            # Despedida inteligente
            goodbye_msg = "O usu√°rio est√° encerrando a sess√£o. Fa√ßa uma despedida amig√°vel e profissional."
            goodbye = self.get_llm_response(goodbye_msg)
            print(f"\nü§ñ: {goodbye}")
            
        except KeyboardInterrupt:
            print(f"\n\nüëã At√© logo!")
        except Exception as e:
            print(f"‚ùå Erro: {e}")

if __name__ == "__main__":
    chatbot = LLMProjectChatbot()
    chatbot.run()